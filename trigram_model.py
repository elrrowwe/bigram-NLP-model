import torch 
import torch.nn.functional as F
from utils import stoi, itos
from nn_model import NN

"""
This file will contain my implementation of the trigram model, based on the code for the bigram one. For now, the file contains a bigram model,
which uses a 'lookup table' (which is actually a tensor, containing probabilities) generated by a 
rather simple neural network. 
"""

words = open('names.txt', 'r').read().splitlines()

#creating the training set 
x = [] 
y = [] #targets
for w in words:
    chars = ['.'] + list(w) + ['.'] #adding special start and end tokens to each word
    for i,j in zip(chars, chars[1:]):
        ind_i = stoi(i)
        ind_j = stoi(j)

        #appending the first char to the predictors list
        x.append(ind_i)  
        y.append(ind_j) #appending the second char to the targets 
        #the model, given the first letter, predicts the next one after it 

#converting previously created lists to tensors 
xt = torch.tensor(x)
yt = torch.tensor(y)

#saving the number of training examples (ngrams, since the model is training on ngrams precisely) for later 
num = xt.nelement()

#encoding the tensors using one-hot encoding 
xtenc = F.one_hot(xt, num_classes=27).float()

#initializing the network
network = NN((27,27), num)

W = network.train(xtenc, yt, learning_rate=50)

#sampling from the model 
for _ in range(5):
    ind = 0
    output = []
    while True:
        xenc = F.one_hot(torch.tensor([ind]), num_classes=27).float() #one-hot encoding the index 
        logits = xenc @ W #basically "plucking out" the row corresponding to the current index out of the weights tensor (because of how matrix multiplication works)
        counts = logits.exp() #converting the log-counts to counts by reversing the log operation (exponentiation)
        p = counts / counts.sum(1, keepdim=True)

        ind = torch.multinomial(p, num_samples=1, replacement=True).item() #sampling a character from the probability distribution tensor
        output.append(itos(ind))
        if ind == 0: #if the index sampled is the end token -- break the loop
            break
    print(''.join(output))