import torch 
import torch.nn.functional as F
from utils import stoi, itos
from nn_model import NN

"""
This file will contain my implementation of the trigram model, based on the code for the bigram one. For now, the file contains a bigram model,
which uses a 'lookup table' (which is actually a tensor, containing probabilities) generated by a 
rather simple neural network. 
"""

#TODO: change the lookup table generation process to a NN-driven one 

words = open('names.txt', 'r').read().splitlines()

#the counts tensor           3D, now that we have 2 predictors and 1 target 
counts_tensor = torch.zeros((27,27,27), dtype=torch.int32)

for w in words:
    chars = ['.'] + list(w) + ['.'] #adding special start and end tokens to each word
    for i,j,k in zip(chars, chars[1:], chars[2:]):
        ind_i = stoi(i)
        ind_j = stoi(j)
        ind_k = stoi(k)

        counts_tensor[ind_i, ind_j, ind_k] += 1

#preprocessing the counts_tensor 'table' for it to contain probabilities 
P_prep = (counts_tensor+5).float()
P_prep /= P_prep.sum(2, keepdim=True) 

#the sampling loop
for _ in range(20):
    ind_1 = 0 #ind of the first predictor
    ind_2 = 0 #ind of the second predictor 
    output = []
    while True:
        p = P_prep[ind_1, ind_2] #the tensor (row of counts_tensor), corresponding to the current letter 

        next_sym = torch.multinomial(p, num_samples=1, replacement=True).item() #sampling a character from the probability distribution tensor

        #the first predictor's index takes on the value of the second predictor's index, and the second predictor's index becomes that of the next letter (symbol)  
        ind_1 = ind_2
        ind_2 = next_sym

        output.append(itos(next_sym))

        if next_sym == 0: #if the index sampled is the end token -- break the loop
            break
    print(''.join(output))


ll = 0.0
n = 0
#calculating the log likelihood of each bigram for model quality estimation 
for w in words:
    chars = ['.'] + list(w) + ['.'] #adding special start and end tokens to each word
    for i,j,k in zip(chars, chars[1:], chars[2:]):
        ind_i = stoi(i)
        ind_j = stoi(j)
        ind_k = stoi(k)

        #the probability of k following i,j OR p(k|i,j) (kind of)
        prob = P_prep[ind_i, ind_j, ind_k]
        prob_log = torch.log(prob).item()
        ll += prob_log
        n += 1

print(f'll: {ll}')
nll = -ll
print(f'nll: {nll}')
print(f'average nll: {nll/n}')